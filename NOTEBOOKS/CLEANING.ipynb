{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Define the directory containing all the data files\n",
    "data_dir = '/home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/DATA/'\n",
    "output_dir = '/home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 text files in /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/DATA/\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all .txt files in the directory\n",
    "file_paths = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "print(f\"Found {len(file_paths)} text files in {data_dir}\")\n",
    "\n",
    "# Initialize a list to store dataframes from each file\n",
    "all_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: ES 09-24.Last.txt\n",
      "Successfully loaded file with 92765 lines\n",
      "Data shape for ES 09-24.Last.txt: (92765, 7)\n",
      "Successfully processed ES 09-24.Last.txt\n",
      "\n",
      "Processing file: ES 06-24.Last.txt\n",
      "Successfully loaded file with 103071 lines\n",
      "Data shape for ES 06-24.Last.txt: (103071, 7)\n",
      "Successfully processed ES 06-24.Last.txt\n",
      "\n",
      "Processing file: ES 09-22.Last.txt\n",
      "Successfully loaded file with 91763 lines\n",
      "Data shape for ES 09-22.Last.txt: (91763, 7)\n",
      "Successfully processed ES 09-22.Last.txt\n",
      "\n",
      "Processing file: ES_03_22.Last.txt\n",
      "Successfully loaded file with 92207 lines\n",
      "Data shape for ES_03_22.Last.txt: (92207, 7)\n",
      "Successfully processed ES_03_22.Last.txt\n",
      "\n",
      "Processing file: ES 12-23.Last.txt\n",
      "Successfully loaded file with 92924 lines\n",
      "Data shape for ES 12-23.Last.txt: (92924, 7)\n",
      "Successfully processed ES 12-23.Last.txt\n",
      "\n",
      "Processing file: ES 06-23.Last.txt\n",
      "Successfully loaded file with 91705 lines\n",
      "Data shape for ES 06-23.Last.txt: (91705, 7)\n",
      "Successfully processed ES 06-23.Last.txt\n",
      "\n",
      "Processing file: ES 12-24.Last.txt\n",
      "Successfully loaded file with 91965 lines\n",
      "Data shape for ES 12-24.Last.txt: (91965, 7)\n",
      "Successfully processed ES 12-24.Last.txt\n",
      "\n",
      "Processing file: ES 03-25.Last.txt\n",
      "Successfully loaded file with 72584 lines\n",
      "Data shape for ES 03-25.Last.txt: (72584, 7)\n",
      "Successfully processed ES 03-25.Last.txt\n",
      "\n",
      "Processing file: ES 12-22.Last.txt\n",
      "Successfully loaded file with 91379 lines\n",
      "Data shape for ES 12-22.Last.txt: (91379, 7)\n",
      "Successfully processed ES 12-22.Last.txt\n",
      "\n",
      "Processing file: ES 03-23.Last.txt\n",
      "Successfully loaded file with 89247 lines\n",
      "Data shape for ES 03-23.Last.txt: (89247, 7)\n",
      "Successfully processed ES 03-23.Last.txt\n",
      "\n",
      "Processing file: ES 06-22.Last.txt\n",
      "Successfully loaded file with 95222 lines\n",
      "Data shape for ES 06-22.Last.txt: (95222, 7)\n",
      "Successfully processed ES 06-22.Last.txt\n",
      "\n",
      "Processing file: ES 09-23.Last.txt\n",
      "Successfully loaded file with 91404 lines\n",
      "Data shape for ES 09-23.Last.txt: (91404, 7)\n",
      "Successfully processed ES 09-23.Last.txt\n",
      "\n",
      "Processing file: ES 03-24.Last.txt\n",
      "Successfully loaded file with 90504 lines\n",
      "Data shape for ES 03-24.Last.txt: (90504, 7)\n",
      "Successfully processed ES 03-24.Last.txt\n"
     ]
    }
   ],
   "source": [
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\nProcessing file: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data_lines = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        continue\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when accessing {file_path}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error when opening file: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Successfully loaded file with {len(data_lines)} lines\")\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    timestamps = []\n",
    "    open_prices = []\n",
    "    high_prices = []\n",
    "    low_prices = []\n",
    "    close_prices = []\n",
    "    volumes = []\n",
    "    contract_names = []  # To track which file/contract the data came from\n",
    "    \n",
    "    # Parse each line\n",
    "    for line in data_lines:\n",
    "        # Split by semicolon\n",
    "        parts = line.strip().split(';')\n",
    "        if len(parts) != 6:\n",
    "            print(f\"Skipping malformed line: {line}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse timestamp (format: YYYYMMDD HHMMSS)\n",
    "        date_time_str = parts[0]\n",
    "        try:\n",
    "            timestamp = datetime.strptime(date_time_str, '%Y%m%d %H%M%S')\n",
    "            timestamps.append(timestamp)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping line with invalid datetime: {line}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse OHLC and volume data\n",
    "        try:\n",
    "            open_prices.append(float(parts[1]))\n",
    "            high_prices.append(float(parts[2]))\n",
    "            low_prices.append(float(parts[3]))\n",
    "            close_prices.append(float(parts[4]))\n",
    "            volumes.append(int(parts[5]))\n",
    "            contract_names.append(file_name.replace('.Last.txt', ''))  # Store contract name\n",
    "        except ValueError:\n",
    "            print(f\"Skipping line with invalid numeric data: {line}\")\n",
    "            continue\n",
    "    \n",
    "    # Skip if no valid data was found\n",
    "    if not timestamps:\n",
    "        print(f\"No valid data found in {file_name}, skipping...\")\n",
    "        continue\n",
    "        \n",
    "    # Create DataFrame for this file\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'open': open_prices,\n",
    "        'high': high_prices,\n",
    "        'low': low_prices,\n",
    "        'close': close_prices,\n",
    "        'volume': volumes,\n",
    "        'contract': contract_names\n",
    "    })\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Add trading session information\n",
    "    df['session'] = 'ETH'  # Default to Extended Trading Hours\n",
    "    # Regular Trading Hours (RTH) is typically 9:30 AM - 4:00 PM ET\n",
    "    df.loc[df.index.hour.isin(range(9, 16)) & \n",
    "           ((df.index.hour != 9) | (df.index.minute >= 30)), 'session'] = 'RTH'\n",
    "    \n",
    "    # Check for data quality issues\n",
    "    print(f\"Data shape for {file_name}: {df.shape}\")\n",
    "    \n",
    "    # Check for duplicated timestamps\n",
    "    duplicates = df.index.duplicated()\n",
    "    if duplicates.any():\n",
    "        print(f\"Found {duplicates.sum()} duplicate timestamps\")\n",
    "        # Either keep first occurrence or handle as needed\n",
    "        df = df[~duplicates]\n",
    "    \n",
    "    # Check for missing data\n",
    "    missing_values = df.isna().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(f\"Missing values per column:\\n{missing_values}\")\n",
    "    \n",
    "    # Check for zero or negative prices\n",
    "    if (df[['open', 'high', 'low', 'close']] <= 0).any().any():\n",
    "        print(\"Warning: Found zero or negative prices\")\n",
    "    \n",
    "    # Check for high-low inconsistency\n",
    "    inconsistent = (df['high'] < df['low']).any()\n",
    "    if inconsistent:\n",
    "        print(\"Warning: Found high < low inconsistencies\")\n",
    "    \n",
    "    # Check for OHLC inconsistencies\n",
    "    ohlc_issues = ((df['open'] > df['high']) | \n",
    "                   (df['open'] < df['low']) | \n",
    "                   (df['close'] > df['high']) | \n",
    "                   (df['close'] < df['low']))\n",
    "    if ohlc_issues.any():\n",
    "        print(f\"Found {ohlc_issues.sum()} OHLC relationship inconsistencies\")\n",
    "    \n",
    "    # Add derived columns useful for analysis\n",
    "    df['bar_range'] = df['high'] - df['low']\n",
    "    df['bar_return'] = df['close'].pct_change()\n",
    "    \n",
    "    # Add this dataframe to our list\n",
    "    all_dfs.append(df)\n",
    "    print(f\"Successfully processed {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Combined Dataset Summary =====\n",
      "Total records: 2373480\n",
      "Date range: 2021-12-05 16:31:00 to 2025-02-27 16:18:00\n",
      "Number of contracts: 13\n",
      "Contracts: ES_03_22, ES 06-22, ES 09-22, ES 12-22, ES 03-23, ES 06-23, ES 09-23, ES 12-23, ES 03-24, ES 06-24, ES 09-24, ES 12-24, ES 03-25\n",
      "RTH sessions: 675972\n",
      "ETH sessions: 1697508\n",
      "\n",
      "Price statistics:\n",
      "               open          high           low         close\n",
      "count  2.373480e+06  2.373480e+06  2.373480e+06  2.373480e+06\n",
      "mean   4.697478e+03  4.698205e+03  4.696745e+03  4.697481e+03\n",
      "std    7.143690e+02  7.142643e+02  7.144513e+02  7.143553e+02\n",
      "min    1.250000e+00  3.507500e+03  1.250000e+00  3.503500e+03\n",
      "25%    4.124250e+03  4.125250e+03  4.123500e+03  4.124250e+03\n",
      "50%    4.494500e+03  4.495250e+03  4.493750e+03  4.494500e+03\n",
      "75%    5.282500e+03  5.283000e+03  5.281750e+03  5.282500e+03\n",
      "max    6.165750e+03  6.166500e+03  6.165250e+03  6.165750e+03\n",
      "\n",
      "Volume statistics:\n",
      "count    2.373480e+06\n",
      "mean     1.086009e+03\n",
      "std      2.326713e+03\n",
      "min      1.000000e+00\n",
      "25%      8.900000e+01\n",
      "50%      2.400000e+02\n",
      "75%      1.296000e+03\n",
      "max      1.854410e+05\n",
      "Name: volume, dtype: float64\n",
      "\n",
      "Combined data saved to: /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/combined_es_futures_data.csv\n",
      "Saved 184414 records for ES_03_22 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES_03_22_data.csv\n",
      "Saved 190444 records for ES 06-22 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 06-22_data.csv\n",
      "Saved 183526 records for ES 09-22 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 09-22_data.csv\n",
      "Saved 182758 records for ES 12-22 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 12-22_data.csv\n",
      "Saved 178494 records for ES 03-23 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 03-23_data.csv\n",
      "Saved 183410 records for ES 06-23 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 06-23_data.csv\n",
      "Saved 182808 records for ES 09-23 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 09-23_data.csv\n",
      "Saved 185848 records for ES 12-23 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 12-23_data.csv\n",
      "Saved 181008 records for ES 03-24 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 03-24_data.csv\n",
      "Saved 206142 records for ES 06-24 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 06-24_data.csv\n",
      "Saved 185530 records for ES 09-24 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 09-24_data.csv\n",
      "Saved 183930 records for ES 12-24 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 12-24_data.csv\n",
      "Saved 145168 records for ES 03-25 to /home/lr/Documents/FUTURUES_PROJECT/futures_vpoc_backtest/RESULTS/cleaned_ES 03-25_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine all dataframes\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, axis=0)\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    combined_df.sort_index(inplace=True)\n",
    "    \n",
    "    print(\"\\n===== Combined Dataset Summary =====\")\n",
    "    print(f\"Total records: {len(combined_df)}\")\n",
    "    print(f\"Date range: {combined_df.index.min()} to {combined_df.index.max()}\")\n",
    "    print(f\"Number of contracts: {combined_df['contract'].nunique()}\")\n",
    "    print(f\"Contracts: {', '.join(combined_df['contract'].unique())}\")\n",
    "    print(f\"RTH sessions: {(combined_df['session'] == 'RTH').sum()}\")\n",
    "    print(f\"ETH sessions: {(combined_df['session'] == 'ETH').sum()}\")\n",
    "    \n",
    "    print(\"\\nPrice statistics:\")\n",
    "    print(combined_df[['open', 'high', 'low', 'close']].describe())\n",
    "    \n",
    "    print(\"\\nVolume statistics:\")\n",
    "    print(combined_df['volume'].describe())\n",
    "    \n",
    "    # Save the combined data\n",
    "    output_path = os.path.join(output_dir, 'combined_es_futures_data.csv')\n",
    "    combined_df.to_csv(output_path)\n",
    "    print(f\"\\nCombined data saved to: {output_path}\")\n",
    "    \n",
    "    # Optional: Save individual cleaned files\n",
    "    for contract in combined_df['contract'].unique():\n",
    "        contract_df = combined_df[combined_df['contract'] == contract]\n",
    "        contract_output_path = os.path.join(output_dir, f'cleaned_{contract}_data.csv')\n",
    "        contract_df.to_csv(contract_output_path)\n",
    "        print(f\"Saved {len(contract_df)} records for {contract} to {contract_output_path}\")\n",
    "else:\n",
    "    print(\"No valid data was processed from any file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
