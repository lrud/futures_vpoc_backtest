"""
Robust Feature Engineering for Financial Time Series
Based on research-backed solutions for stable neural network training

Key Features:
- Rank-based target transformation (bounded 0-1, eliminates outliers)
- Top 5 most important features from 1.13M sample analysis
- Memory-efficient chunked processing
- ROCm 7 consumer GPU optimization
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Union
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Phase 1 Technical Analysis Libraries
import talib
import pandas_ta as ta
from arch import arch_model

# Add project root to path
sys.path.append('/workspace')

from src.core.data import FuturesDataManager
from src.utils.logging import get_logger
from src.config.settings import settings

# Initialize logger
logger = get_logger(__name__)

class RobustFeatureEngineer:
    """
    Research-backed feature engineering implementing proven solutions:
    1. Rank-based target transformation (eliminates Â±13% outliers)
    2. Top 5 most important features from statistical analysis
    3. Chunked processing for memory efficiency
    4. ROCm 7 consumer GPU optimization
    """

    # Phase 1 Enhanced Feature Set (28+ sophisticated features)
    # Technical Indicators + Advanced Volatility + Time-Based Features + Statistically Confirmed Features
    PHASE1_FEATURES = [
        # === STATISTICALLY CONFIRMED HIGH-PREDICTIVE FEATURES (6) ===
        'price_change_1',       # Price Change 1-period (r=-0.9788) â­ EXTREMELY STRONG
        'price_change_3',       # Price Change 3-period (r=-0.9613) â­ EXTREMELY STRONG
        'price_vs_5_ma',        # Price vs 5-period MA (r=-0.9464) â­ EXTREMELY STRONG
        'volume_vs_5_ma',       # Volume vs 5-period MA (r=0.0908) â­ MODERATE
        'volume_vs_20_ma',      # Volume vs 20-period MA (r=0.0726) â­ MODERATE
        'volume_change_5',      # Volume Change 5-period (r=0.0369) â­ MODERATE

        # === TECHNICAL INDICATORS (8) ===
        'rsi_14',               # Relative Strength Index (momentum oscillator)
        'macd_line',            # MACD line (trend following)
        'macd_signal',          # MACD signal line (crossover signals)
        'macd_histogram',       # MACD histogram (momentum strength) r=-0.5322 â­ STRONGEST
        'stoch_k',              # Stochastic %K (overbought/oversold)
        'stoch_d',              # Stochastic %D (signal line)
        'atr_14',               # Average True Range (volatility)
        'bb_position',          # Bollinger Band position (price relative position)

        # === ADVANCED VOLATILITY FEATURES (8) ===
        'realized_vol_daily',   # Daily realized volatility (minute data)
        'bipower_var_daily',    # Bipower variation (jump-robust)
        'realized_jump_var',    # Realized jump variance (jump component)
        'har_1d',              # HAR 1-day volatility (short-term)
        'har_5d',              # HAR 5-day volatility (medium-term)
        'har_22d',             # HAR 22-day volatility (long-term)
        'garch_vol',           # GARCH(1,1) conditional volatility
        'vol_regime',          # Volatility regime flag (high/low)

        # === TIME-BASED FEATURES (3) ===
        'day_of_week',         # Day of week (0=Mon, 4=Fri)
        'session_indicator',    # Session type (from existing data)
        'time_of_day',         # Intraday time feature (normalized)

        # === MACRO FEATURES (1) ===
        'vix',                 # VIX index (already present)

        # === SELECTED ORIGINAL FEATURES (2) ===
        'close_change_pct',     # Keep most predictive basic feature
        'vwap'                  # Keep VWAP as baseline
    ]

    # Legacy compatibility
    ENHANCED_FEATURES = PHASE1_FEATURES

    def __init__(self, device_ids: Optional[List[int]] = None, chunk_size: int = 100000):
        """
        Initialize robust feature engineer.

        Args:
            device_ids: List of GPU device IDs for distributed processing
            chunk_size: Chunk size for memory-efficient processing
        """
        self.device_ids = device_ids or []
        self.chunk_size = chunk_size
        self.feature_columns = self.ENHANCED_FEATURES
        self.target_stats = {}  # Store for inverse transform

        logger.info("ğŸš€ Initializing Robust Feature Engineering")
        logger.info(f"  â€¢ Features: {', '.join(self.feature_columns)}")
        logger.info(f"  â€¢ Chunk size: {chunk_size:,}")
        logger.info(f"  â€¢ GPUs: {self.device_ids if self.device_ids else 'CPU only'}")

    def create_features_robust(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Create robust features using chunked processing for memory efficiency.

        Args:
            data: Raw OHLCV data with VIX

        Returns:
            DataFrame with engineered features
        """
        logger.info(f"ğŸ”§ Creating robust features for {len(data):,} rows...")

        # Process in chunks to manage memory for large datasets
        total_chunks = (len(data) + self.chunk_size - 1) // self.chunk_size
        features_list = []

        for chunk_idx in range(total_chunks):
            start_idx = chunk_idx * self.chunk_size
            end_idx = min((chunk_idx + 1) * self.chunk_size, len(data))
            chunk_data = data.iloc[start_idx:end_idx].copy()

            # Create features for this chunk
            chunk_features = self._create_chunk_features(chunk_data)
            features_list.append(chunk_features)

            # Progress update
            if (chunk_idx + 1) % 5 == 0 or chunk_idx == total_chunks - 1:
                logger.info(f"  Processed chunk {chunk_idx + 1}/{total_chunks} ({end_idx:,}/{len(data):,} rows)")

        # Concatenate all chunks
        features = pd.concat(features_list, ignore_index=False)

        # Clean up memory
        del features_list
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

        logger.info(f"âœ… Created {len(features.columns)} robust features")
        return features

    def _create_chunk_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Create Phase 1 enhanced features for a single chunk of data.
        Integrates technical indicators, volatility features, and time-based features.
        """
        logger.info(f"ğŸ”§ Creating Phase 1 enhanced features for chunk with {len(data)} rows")

        features = pd.DataFrame(index=data.index)

        # 1. Basic price and volume features (keep most predictive originals)
        features['close_change_pct'] = data['close'].pct_change()
        features['price_range'] = (data['high'] - data['low']) / data['close']
        features['volume_change_1'] = data['volume'].pct_change() if 'volume' in data.columns else 0.0

        # 1b. ADD STATISTICALLY CONFIRMED HIGH-PREDICTIVE FEATURES
        logger.info("ğŸ¯ Adding statistically confirmed predictive features...")

        # Price Features (HIGHLY SIGNIFICANT - r > -0.94)
        features['price_change_1'] = data['close'].pct_change(1)  # r=-0.9788 (EXTREMELY STRONG)
        features['price_change_3'] = data['close'].pct_change(3)  # r=-0.9613 (EXTREMELY STRONG)

        # Price vs Moving Average Features (HIGHLY SIGNIFICANT)
        price_ma_5 = data['close'].rolling(5).mean()
        features['price_vs_5_ma'] = (data['close'] - price_ma_5) / price_ma_5  # r=-0.9464 (EXTREMELY STRONG)

        # Volume Features (MODERATELY SIGNIFICANT - r > 0.03)
        if 'volume' in data.columns:
            volume_ma_5 = data['volume'].rolling(5).mean()
            volume_ma_20 = data['volume'].rolling(20).mean()
            features['volume_vs_5_ma'] = data['volume'] / volume_ma_5 - 1  # r=0.0908 (MODERATE)
            features['volume_vs_20_ma'] = data['volume'] / volume_ma_20 - 1  # r=0.0726 (MODERATE)
            features['volume_change_5'] = data['volume'].pct_change(5)  # r=0.0369 (MODERATE)
        else:
            features['volume_vs_5_ma'] = 0.0
            features['volume_vs_20_ma'] = 0.0
            features['volume_change_5'] = 0.0

        logger.info("âœ… Added 6 statistically confirmed predictive features")

        # 2. VWAP features (high predictive power)
        typical_price = (data['high'] + data['low'] + data['close']) / 3
        features['vwap'] = typical_price

        # 3. Add Phase 1 Technical Indicators
        logger.info("ğŸ“ˆ Adding technical indicators...")
        try:
            technical_features = self._calculate_technical_indicators(data)
            features = pd.concat([features, technical_features], axis=1)
            logger.info(f"âœ… Added technical indicators: {list(technical_features.columns)}")
        except Exception as e:
            logger.warning(f"âš ï¸ Technical indicators failed: {e}")
            # Add placeholder NaN columns to maintain structure
            for tech_col in ['rsi_14', 'macd_line', 'macd_signal', 'macd_histogram',
                           'stoch_k', 'stoch_d', 'atr_14', 'bb_position']:
                features[tech_col] = np.nan

        # 4. Add Phase 1 Volatility Features
        logger.info("ğŸ“Š Adding volatility features...")
        try:
            volatility_features = self._calculate_volatility_features(data)
            features = pd.concat([features, volatility_features], axis=1)
            logger.info(f"âœ… Added volatility features: {list(volatility_features.columns)}")
        except Exception as e:
            logger.warning(f"âš ï¸ Volatility features failed: {e}")
            # Add placeholder NaN columns
            for vol_col in ['realized_vol_daily', 'bipower_var_daily', 'realized_jump_var',
                          'har_1d', 'har_5d', 'har_22d', 'garch_vol', 'vol_regime']:
                features[vol_col] = np.nan

        # 5. Add Time-Based Features
        logger.info("â° Adding time-based features...")
        try:
            time_features = self._calculate_time_features(data)
            features = pd.concat([features, time_features], axis=1)
            logger.info(f"âœ… Added time features: {list(time_features.columns)}")
        except Exception as e:
            logger.warning(f"âš ï¸ Time features failed: {e}")
            # Add placeholder NaN columns
            for time_col in ['day_of_week', 'time_of_day']:
                features[time_col] = np.nan

        # 6. Add Session Indicator (if available)
        if 'session' in data.columns:
            features['session_indicator'] = data['session']
        else:
            features['session_indicator'] = 1  # Default to regular session

        # 7. Add VIX (if available)
        if 'VIX' in data.columns:
            features['vix'] = data['VIX']
        else:
            features['vix'] = 20.0  # Default VIX level

        # 8. Ensure all required Phase 1 features exist
        required_features = self.feature_columns  # This should be PHASE1_FEATURES now
        missing_features = [f for f in required_features if f not in features.columns]
        if missing_features:
            logger.warning(f"âš ï¸ Missing Phase 1 features: {missing_features}")
            for missing in missing_features:
                features[missing] = np.nan

        # 9. Select only the features we need in the correct order
        available_features = [col for col in self.feature_columns if col in features.columns]
        result = features[available_features].copy()

        # Remove intermediate columns that shouldn't be in the final feature set
        intermediate_columns = ['bar_return', 'date']
        for col in intermediate_columns:
            if col in result.columns:
                result.drop(col, axis=1, inplace=True)

        logger.info(f"âœ… Phase 1 feature creation complete: {len(result.columns)} features for {len(result)} rows")
        logger.info(f"ğŸ“Š Features created: {list(result.columns)}")

        return result

    def _calculate_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate Phase 1 technical indicators using TA-Lib.

        Args:
            data: OHLCV DataFrame

        Returns:
            DataFrame with technical indicators
        """
        try:
            indicators = pd.DataFrame(index=data.index)

            # RSI (14)
            indicators['rsi_14'] = talib.RSI(data['close'].values, timeperiod=14)

            # MACD (12, 26, 9)
            macd, macd_signal, macd_hist = talib.MACD(
                data['close'].values, fastperiod=12, slowperiod=26, signalperiod=9
            )
            indicators['macd_line'] = macd
            indicators['macd_signal'] = macd_signal
            indicators['macd_histogram'] = macd_hist

            # Stochastic Oscillator (%K 14, %D 3)
            slowk, slowd = talib.STOCH(
                data['high'].values, data['low'].values, data['close'].values,
                fastk_period=14, slowk_period=3, slowd_period=3
            )
            indicators['stoch_k'] = slowk
            indicators['stoch_d'] = slowd

            # ATR (14)
            indicators['atr_14'] = talib.ATR(
                data['high'].values, data['low'].values, data['close'].values, timeperiod=14
            )

            # Bollinger Bands Position (Normalized within bands)
            upper, middle, lower = talib.BBANDS(
                data['close'].values, timeperiod=20, nbdevup=2, nbdevdn=2
            )
            # Avoid division by zero
            band_width = upper - lower
            indicators['bb_position'] = np.where(
                band_width > 0, (data['close'].values - middle) / band_width, 0
            )

            logger.info("âœ… Calculated technical indicators: RSI, MACD, Stochastic, ATR, Bollinger Bands")
            return indicators

        except Exception as e:
            logger.error(f"âŒ Technical indicator calculation failed: {e}")
            # Return empty DataFrame with correct columns
            return pd.DataFrame(index=data.index, columns=[
                'rsi_14', 'macd_line', 'macd_signal', 'macd_histogram',
                'stoch_k', 'stoch_d', 'atr_14', 'bb_position'
            ])

    def _calculate_volatility_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate advanced volatility features using daily grouping from minute data.

        Args:
            data: OHLCV DataFrame with date column

        Returns:
            DataFrame with volatility features
        """
        try:
            vol_features = pd.DataFrame(index=data.index)

            # Calculate minute returns
            minute_returns = data['close'].pct_change()
            vol_features['bar_return'] = minute_returns

            # Add date column for merging
            if 'date' not in vol_features.columns:
                if 'date' in data.columns:
                    vol_features['date'] = data['date']
                else:
                    vol_features['date'] = data.index.date

            # Ensure date column exists and is datetime
            if 'date' not in data.columns:
                if 'timestamp' in data.columns:
                    data['date'] = pd.to_datetime(data['timestamp']).dt.date
                else:
                    data['date'] = data.index.date

            # Group by date for daily volatility calculations
            daily_groups = data.groupby('date')

            # Realized Volatility (square root of sum of squared returns)
            realized_vol = daily_groups['bar_return'].apply(
                lambda x: np.sqrt(np.sum(np.square(x.dropna())))
            )

            # Bipower Variation (jump-robust volatility)
            bipower_var = daily_groups['bar_return'].apply(
                lambda x: (np.pi/2) * (1/(len(x)-1)) *
                         np.sum(np.abs(x.dropna().iloc[:-1]) * np.abs(x.dropna().iloc[1:]))
            )

            # Merge back to main dataframe
            vol_features = vol_features.merge(
                realized_vol.rename('realized_vol_daily'),
                left_on='date', right_index=True, how='left'
            )
            vol_features = vol_features.merge(
                bipower_var.rename('bipower_var_daily'),
                left_on='date', right_index=True, how='left'
            )

            # Realized Jump Variance (positive part only)
            vol_features['realized_jump_var'] = np.maximum(0,
                vol_features['realized_vol_daily'] - vol_features['bipower_var_daily']
            )

            # HAR Features (Heterogeneous Autoregressive)
            vol_features['har_1d'] = vol_features['realized_vol_daily'].rolling(1).mean()
            vol_features['har_5d'] = vol_features['realized_vol_daily'].rolling(5).mean()
            vol_features['har_22d'] = vol_features['realized_vol_daily'].rolling(22).mean()

            # Volatility Regime Flag (high/low based on rolling percentiles)
            vol_25 = vol_features['realized_vol_daily'].rolling(60).quantile(0.25)
            vol_75 = vol_features['realized_vol_daily'].rolling(60).quantile(0.75)
            vol_features['vol_regime'] = (vol_features['realized_vol_daily'] > vol_75).astype(int)

            # GARCH(1,1) Conditional Volatility (sample calculation for performance)
            try:
                # Use a sample for GARCH to avoid computational overhead
                sample_returns = minute_returns.dropna().iloc[-1000:] * 100  # Scale to % for stability
                if len(sample_returns) > 50:  # Minimum data for GARCH
                    model = arch_model(sample_returns, vol='Garch', p=1, q=1, disp='off')
                    res = model.fit(disp='off')

                    # Create a mapping from sample indices to full dataframe indices
                    sample_indices = minute_returns.dropna().iloc[-1000:].index
                    full_length = len(minute_returns)

                    # Map conditional volatility back to full dataframe
                    garch_series = pd.Series(res.conditional_volatility, index=sample_indices)
                    vol_features.loc[garch_series.index, 'garch_vol'] = garch_series

                    logger.info("âœ… Calculated GARCH(1,1) conditional volatility")
                else:
                    logger.warning("âš ï¸ Insufficient data for GARCH modeling")
                    vol_features['garch_vol'] = np.nan

            except Exception as e:
                logger.warning(f"âš ï¸ GARCH modeling failed: {e}")
                vol_features['garch_vol'] = np.nan

            logger.info("âœ… Calculated volatility features: Realized Volatility, Bipower Variation, HAR, GARCH, Volatility Regime")
            return vol_features

        except Exception as e:
            logger.error(f"âŒ Volatility feature calculation failed: {e}")
            # Return empty DataFrame with correct columns
            return pd.DataFrame(index=data.index, columns=[
                'realized_vol_daily', 'bipower_var_daily', 'realized_jump_var',
                'har_1d', 'har_5d', 'har_22d', 'garch_vol', 'vol_regime'
            ])

    def _calculate_time_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate time-based features for intraday patterns.

        Args:
            data: OHLCV DataFrame with timestamp

        Returns:
            DataFrame with time-based features
        """
        try:
            time_features = pd.DataFrame(index=data.index)

            # Convert timestamp to datetime if not already
            if 'timestamp' in data.columns:
                timestamps = pd.to_datetime(data['timestamp'])
                # Day of week (0=Monday, 4=Friday)
                time_features['day_of_week'] = timestamps.dt.dayofweek
                # Time of day (normalized 0-1, where 0.5 = 12:00 PM)
                time_features['time_of_day'] = timestamps.dt.hour / 24.0 + timestamps.dt.minute / 1440.0
                # Session classification
                hour = timestamps.dt.hour
            else:
                timestamps = data.index
                # Day of week (0=Monday, 4=Friday) - DatetimeIndex doesn't need .dt
                time_features['day_of_week'] = timestamps.dayofweek
                # Time of day (normalized 0-1, where 0.5 = 12:00 PM)
                time_features['time_of_day'] = timestamps.hour / 24.0 + timestamps.minute / 1440.0
                # Session classification
                hour = timestamps.hour

            # Session indicator (if session column exists)
            if 'session' in data.columns:
                time_features['session_indicator'] = data['session']
            else:
                # Default session classification based on time
                # Asia: 20:00-06:59, Europe: 07:00-12:00, NY: 13:00-19:59
                time_features['session_indicator'] = np.where(
                    (hour >= 20) | (hour < 7), 'ASIA',
                    np.where((hour >= 7) & (hour < 13), 'EUROPE', 'US')
                )

            logger.info("âœ… Calculated time features: Day-of-Week, Time-of-Day, Session")
            return time_features

        except Exception as e:
            logger.error(f"âŒ Time feature calculation failed: {e}")
            # Return empty DataFrame with correct columns
            return pd.DataFrame(index=data.index, columns=[
                'day_of_week', 'time_of_day', 'session_indicator'
            ])

    def create_target_directional(self, data: pd.DataFrame, threshold: float = 0.001) -> pd.Series:
        """
        Create directional target for binary classification.

        This transformation:
        - Creates clear binary directional targets (UP=1, DOWN=0)
        - Uses configurable threshold to filter noise
        - Focuses on meaningful price movements
        - Eliminates mean-reversion bias

        Args:
            data: Raw OHLCV data
            threshold: Minimum price change threshold (default 0.1%)

        Returns:
            Binary target series (0 for DOWN, 1 for UP)
        """
        logger.info("ğŸ¯ Creating directional target for binary classification...")

        # Calculate forward-looking returns (5-period lookahead)
        future_returns = data['close'].pct_change(5).shift(-5)

        # Remove NaN values
        future_returns = future_returns.dropna()

        logger.info(f"  â€¢ Future returns range: [{future_returns.min():.6f}, {future_returns.max():.6f}]")
        logger.info(f"  â€¢ Mean return: {future_returns.mean():.6f}")
        logger.info(f"  â€¢ Threshold: Â±{threshold:.3f} ({threshold*100:.2f}%)")

        # Create binary directional targets
        # 1 = UP movement (return > threshold)
        # 0 = DOWN movement (return < -threshold)
        # Filter out small movements between -threshold and +threshold
        target_directional = np.where(
            future_returns > threshold, 1,      # UP signal
            np.where(future_returns < -threshold, 0, np.nan)  # DOWN signal, with filter
        )

        # Remove NaN values (filtered small movements)
        valid_mask = ~np.isnan(target_directional)
        target_clean = target_directional[valid_mask]
        returns_clean = future_returns[valid_mask]

        # Calculate statistics
        up_signals = np.sum(target_clean == 1)
        down_signals = np.sum(target_clean == 0)
        total_signals = len(target_clean)

        # Store transformation parameters
        self.target_stats = {
            'threshold': threshold,
            'up_signals': int(up_signals),
            'down_signals': int(down_signals),
            'total_signals': int(total_signals),
            'up_percentage': float(up_signals / total_signals * 100),
            'mean_up_return': float(returns_clean[target_clean == 1].mean()),
            'mean_down_return': float(returns_clean[target_clean == 0].mean()),
            'transform_type': 'directional_binary'
        }

        logger.info(f"  â€¢ UP signals: {up_signals:,} ({up_signals/total_signals*100:.1f}%)")
        logger.info(f"  â€¢ DOWN signals: {down_signals:,} ({down_signals/total_signals*100:.1f}%)")
        logger.info(f"  â€¢ Mean UP return: {self.target_stats['mean_up_return']:.6f}")
        logger.info(f"  â€¢ Mean DOWN return: {self.target_stats['mean_down_return']:.6f}")
        logger.info(f"  â€¢ Final target samples: {total_signals:,}")
        logger.info("âœ… Directional target created - binary classification ready")

        return pd.Series(target_clean, index=returns_clean.index)

    def inverse_transform_target(self, transformed_target: np.ndarray) -> np.ndarray:
        """
        Inverse transform target from 0-1 range back to approximate returns.

        Note: This is approximate since rank transform is not perfectly invertible.
        Uses quantile-based approach to map percentiles back to return space.

        Args:
            transformed_target: Transformed target values (0-1 range)

        Returns:
            Approximate raw returns
        """
        if not self.target_stats:
            logger.warning("No target transformation statistics available")
            return transformed_target

        # Use quantile-based mapping for approximate inverse
        # This maps the percentile back to an approximate return value
        quantiles = np.array([0.01, 0.25, 0.5, 0.75, 0.99])  # Key quantiles

        # For a simple approximation, we can use linear mapping
        # This is not perfectly accurate but gives reasonable estimates
        min_return = self.target_stats['min_return']
        max_return = self.target_stats['max_return']

        approximate_returns = transformed_target * (max_return - min_return) + min_return

        return approximate_returns

    def scale_features(self, features: pd.DataFrame) -> Tuple[np.ndarray, Dict]:
        """
        Scale features using robust unit scaling.

        Uses robust statistics (median and IQR) to avoid outlier influence.

        Args:
            features: Feature DataFrame

        Returns:
            Tuple of (scaled_features, scaling_params)
        """
        logger.info("ğŸ“Š Scaling features with robust statistics...")

        scaling_params = {}
        scaled_features = []

        for col in features.columns:
            col_data = features[col].dropna()

            # Robust scaling using median and IQR
            median_val = col_data.median()
            q1 = col_data.quantile(0.25)
            q3 = col_data.quantile(0.75)
            iqr = q3 - q1

            # Avoid division by zero
            scale_val = iqr if iqr > 0 else 1.0

            # Scale: (x - median) / IQR
            scaled_col = (features[col] - median_val) / scale_val

            # Store scaling parameters
            scaling_params[col] = {
                'median': median_val,
                'q1': q1,
                'q3': q3,
                'iqr': iqr,
                'scale': scale_val
            }

            scaled_features.append(scaled_col)

        # Convert to numpy array
        scaled_array = np.column_stack(scaled_features)

        logger.info(f"âœ… Scaled {len(features.columns)} features using robust statistics")

        return scaled_array, scaling_params

    def load_and_prepare_data_robust(self, data_path: str, device_ids: Optional[List[int]] = None,
                                   data_fraction: float = 1.0, chunk_size: int = 15000) -> Optional[Tuple]:
        """
        Complete robust data preparation pipeline.

        Implements all research-backed solutions for stable training:
        1. Load and clean data
        2. Create robust features (top 5 important features)
        3. Create robust target (rank-based transformation)
        4. Scale features robustly
        5. Prepare for distributed training

        Args:
            data_path: Path to data file
            device_ids: GPU device IDs for distributed processing
            data_fraction: Fraction of data to use (1.0 = full dataset)
            chunk_size: Chunk size for VPOC processing

        Returns:
            Tuple of (X_train, y_train, X_val, y_val, feature_columns, scaling_params, target_stats)
            or None if failed
        """
        try:
            logger.info("ğŸš€ Starting robust data preparation pipeline...")

            # Load data using existing data manager
            data_manager = FuturesDataManager()
            data = data_manager.load_futures_data_file(data_path)

            if data is None or data.empty:
                logger.error("âŒ Failed to load data")
                return None

            # Sample data if requested
            if data_fraction < 1.0:
                n_samples = int(len(data) * data_fraction)
                data = data.iloc[:n_samples]
                logger.info(f"ğŸ“Š Using {data_fraction:.1%} of data: {len(data):,} rows")

            # Create robust features
            logger.info("ğŸ”§ Creating robust features...")
            features = self.create_features_robust(data)

            # DEBUG: Verify feature engineering completed
            logger.info(f"ğŸ› DEBUG: Feature engineering completed successfully")
            logger.info(f"ğŸ› DEBUG: Features shape: {features.shape}")
            logger.info(f"ğŸ› DEBUG: Features columns: {list(features.columns)}")
            logger.info(f"ğŸ› DEBUG: Features dtypes: {features.dtypes.value_counts().to_dict()}")

            # DEBUG: Analyze NaN values by feature
            nan_counts = features.isna().sum()
            total_nans = nan_counts.sum()
            logger.info(f"ğŸ› DEBUG: Total NaN count in features: {total_nans}")

            if total_nans > 0:
                logger.info("ğŸ› DEBUG: NaN breakdown by feature:")
                for feature, nan_count in nan_counts[nan_counts > 0].sort_values(ascending=False).items():
                    nan_percentage = (nan_count / len(features)) * 100
                    logger.info(f"   â€¢ {feature}: {nan_count:,} NaNs ({nan_percentage:.1f}%)")

                # Check first few rows to see NaN pattern
                logger.info("ğŸ› DEBUG: NaN pattern in first 5 rows:")
                for i in range(min(5, len(features))):
                    nan_features = features.columns[features.iloc[i].isna()].tolist()
                    if nan_features:
                        logger.info(f"   Row {i}: NaN in {len(nan_features)} features: {nan_features[:10]}{'...' if len(nan_features) > 10 else ''}")

            logger.info(f"ğŸ› DEBUG: Overall NaN rate: {(total_nans / (features.shape[0] * features.shape[1])) * 100:.1f}%")

            # Create directional target (binary classification)
            logger.info("ğŸ¯ Creating directional target...")
            try:
                logger.info(f"ğŸ› DEBUG: About to call create_target_directional...")
                target = self.create_target_directional(data)
                logger.info(f"âœ… Target created successfully: {len(target)} values")
                logger.info(f"ğŸ› DEBUG: Target shape: {target.shape}")
                logger.info(f"ğŸ› DEBUG: Target NaN count: {target.isna().sum()}")
            except Exception as e:
                logger.error(f"âŒ Target creation failed: {e}")
                logger.error(f"ğŸ› DEBUG: Exception during target creation: {type(e).__name__}: {e}")
                import traceback
                logger.error(f"ğŸ› DEBUG: Traceback: {traceback.format_exc()}")
                raise

            # Align features and target
            logger.info(f"ğŸ› DEBUG: About to align features and target...")
            logger.info(f"ğŸ› DEBUG: Features index length: {len(features.index)}")
            logger.info(f"ğŸ› DEBUG: Target index length: {len(target.index)}")

            common_index = features.index.intersection(target.index)
            logger.info(f"ğŸ› DEBUG: Common index length: {len(common_index)}")

            features = features.loc[common_index]
            target = target.loc[common_index]
            logger.info(f"ğŸ› DEBUG: After alignment - Features: {features.shape}, Target: {target.shape}")

            # Robust NaN handling for financial features
            logger.info("ğŸ§¹ Implementing robust NaN handling for financial features...")
            logger.info(f"ğŸ› DEBUG: NaN check before cleaning - Features: {features.isna().sum().sum()}, Target: {target.isna().sum()}")

            # Instead of dropping all NaN rows, use financial-appropriate NaN handling
            features_clean = features.copy()

            # Strategy 1: Forward fill for most technical indicators (appropriate for time series)
            features_clean = features_clean.fillna(method='ffill', limit=5)

            # Strategy 2: Backward fill for remaining NaNs at the beginning
            features_clean = features_clean.fillna(method='bfill', limit=1)

            # Strategy 3: Fill remaining NaNs with feature medians (robust to outliers)
            for col in features_clean.columns:
                if features_clean[col].isna().any():
                    median_val = features_clean[col].median()
                    if not pd.isna(median_val):
                        features_clean[col] = features_clean[col].fillna(median_val)
                    else:
                        # If median is also NaN, use 0
                        features_clean[col] = features_clean[col].fillna(0)

            # Strategy 4: Only drop rows that still have NaNs after all imputation
            mask = ~(features_clean.isna().any(axis=1) | target.isna())
            features_final = features_clean[mask]
            target_final = target[mask]

            logger.info(f"âœ… Final clean dataset: {len(features_final):,} samples, {len(features_final.columns)} features")
            logger.info(f"ğŸ› DEBUG: After robust cleaning - Features: {features_final.shape}, Target: {target_final.shape}")

            # Verify we still have data
            if len(features_final) == 0:
                logger.error("âŒ No data remaining after NaN handling!")
                logger.error(f"ğŸ› DEBUG: Original NaN count: {features.isna().sum().sum()}")
                logger.error(f"ğŸ› DEBUG: Remaining NaN count after imputation: {features_clean.isna().sum().sum()}")
                raise ValueError("All data lost during NaN cleaning - check feature engineering")

            # Update features and target for processing
            features = features_final
            target = target_final

            # Handle categorical features (convert to numeric)
            logger.info("ğŸ”„ Converting categorical features to numeric...")
            categorical_features = []
            for col in features.columns:
                if features[col].dtype == 'object':
                    categorical_features.append(col)
                    unique_vals = features[col].unique()
                    logger.info(f"ğŸ› DEBUG: Converting categorical feature '{col}': {unique_vals}")

                    # Convert categorical to numeric using label encoding
                    if col == 'session_indicator':
                        # Special handling for session_indicator: ETH=0, RTH=1
                        mapping = {'ETH': 0, 'RTH': 1}
                        features[col] = features[col].map(mapping).fillna(0)
                        logger.info(f"âœ… Converted {col}: ETHâ†’0, RTHâ†’1")
                    else:
                        # Generic label encoding for other categorical features
                        from sklearn.preprocessing import LabelEncoder
                        le = LabelEncoder()
                        features[col] = le.fit_transform(features[col].astype(str))
                        logger.info(f"âœ… Converted {col} using label encoding")

            # DEBUG: Check for any remaining non-numeric features
            logger.info("ğŸ” DEBUG: Final check for non-numeric features...")
            non_numeric_features = []
            for col in features.columns:
                if features[col].dtype == 'object':
                    unique_vals = features[col].unique()
                    logger.error(f"ğŸ› DEBUG: Still non-numeric feature '{col}': {unique_vals}")
                    non_numeric_features.append(col)

            if non_numeric_features:
                logger.error(f"âŒ Found {len(non_numeric_features)} remaining non-numeric features: {non_numeric_features}")
                raise ValueError(f"Non-numeric features found: {non_numeric_features}")

            # Scale features
            logger.info("ğŸ“ Scaling features...")
            try:
                logger.info(f"ğŸ› DEBUG: About to call scale_features with input shape: {features.shape}")
                X_scaled, scaling_params = self.scale_features(features)
                logger.info(f"âœ… Features scaled successfully: {X_scaled.shape}")
                logger.info(f"ğŸ› DEBUG: Scaling params keys: {list(scaling_params.keys()) if scaling_params else 'None'}")
            except Exception as e:
                logger.error(f"âŒ Feature scaling failed: {e}")
                logger.error(f"ğŸ› DEBUG: Exception during scaling: {type(e).__name__}: {e}")
                import traceback
                logger.error(f"ğŸ› DEBUG: Scaling traceback: {traceback.format_exc()}")
                raise
            y = target.values

            # Split data (80% train, 20% validation)
            split_idx = int(len(X_scaled) * 0.8)

            X_train = X_scaled[:split_idx]
            y_train = y[:split_idx]
            X_val = X_scaled[split_idx:]
            y_val = y[split_idx:]

            logger.info(f"âœ… Data split: Train={len(X_train):,}, Val={len(X_val):,}")
            logger.info(f"âœ… Target distribution: Train=[{y_train.min():.3f}, {y_train.max():.3f}], Val=[{y_val.min():.3f}, {y_val.max():.3f}]")

            return (
                X_train, y_train, X_val, y_val,
                list(features.columns),  # feature_names
                scaling_params,
                self.target_stats
            )

        except Exception as e:
            logger.error(f"âŒ Robust data preparation failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

def test_robust_feature_engineering():
    """Test the robust feature engineering pipeline."""
    logger.info("ğŸ§ª Testing Robust Feature Engineering...")

    # Initialize robust feature engineer
    feature_engineer = RobustFeatureEngineer(chunk_size=50000)

    # Load test data
    data_path = "/workspace/DATA/MERGED/merged_es_vix_test.csv"

    # Prepare data (using smaller fraction for testing)
    result = feature_engineer.load_and_prepare_data_robust(
        data_path=data_path,
        data_fraction=0.1,  # Use 10% for testing
        chunk_size=15000
    )

    if result is None:
        logger.error("âŒ Robust feature engineering test failed")
        return False

    X_train, y_train, X_val, y_val, feature_names, scaling_params, target_stats = result

    # Basic validation
    logger.info("âœ… Basic validation checks:")
    logger.info(f"  â€¢ Training data shape: {X_train.shape}")
    logger.info(f"  â€¢ Validation data shape: {X_val.shape}")
    logger.info(f"  â€¢ Feature names: {feature_names}")
    logger.info(f"  â€¢ Target range: [{y_train.min():.3f}, {y_train.max():.3f}]")
    logger.info(f"  â€¢ No NaN values: {not np.any(np.isnan(X_train)) and not np.any(np.isnan(y_train))}")
    logger.info(f"  â€¢ Target transformation: {target_stats.get('transform_type', 'unknown')}")

    # Test inverse transform
    test_target = np.array([0.1, 0.5, 0.9])
    inverse_target = feature_engineer.inverse_transform_target(test_target)
    logger.info(f"  â€¢ Inverse transform test: {test_target} â†’ {inverse_target}")

    logger.info("ğŸ‰ Robust Feature Engineering test completed successfully!")
    return True

if __name__ == "__main__":
    success = test_robust_feature_engineing()
    if success:
        print("âœ… Robust Feature Engineering is ready for use!")
    else:
        print("âŒ Robust Feature Engineering test failed!")